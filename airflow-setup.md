
# ğŸš€ dbt + Airflow + Elementary â€“ Local Workflow Guide

Este documento resume el flujo de trabajo local para ejecutar pipelines de dbt con Airflow y Elementary, usando Docker.

---

## ğŸ“¦ Requisitos previos

- Docker y Docker Compose instalados
- Este repositorio clonado localmente
- Archivo `docker-compose.yml` en la raÃ­z del proyecto

---

## ğŸ§­ NavegaciÃ³n

Abre una terminal y dirÃ­gete al directorio raÃ­z del proyecto:

```bash
cd ~/Developer/data-engineering-env
````

---

## ğŸ” Primer uso (o tras limpiar todo)

Si es la primera vez que usas el entorno o has hecho un `docker compose down -v`:

```bash
docker compose build                # (Re)construye la imagen personalizada con git/dbt/edr
docker compose up airflow-init     # Inicializa la DB de Airflow y crea el usuario admin
```

---

## ğŸš€ Inicio habitual

Para iniciar todo el stack de servicios (Airflow, PostgreSQL, etc.):

```bash
docker compose up -d
```

Esto levanta en segundo plano:

* Airflow Webserver
* Airflow Scheduler
* Airflow metadata Postgres
* PostgreSQL de dbt

---

## ğŸ–¥ Interfaz de usuario de Airflow

Accede desde tu navegador a:

ğŸ‘‰ [http://localhost:8080](http://localhost:8080)

Usa estas credenciales:

* **Usuario:** `admin`
* **ContraseÃ±a:** `admin`

Pasos:

1. Despausa el DAG `dbt_pipeline` si estÃ¡ pausado
2. Ejecuta el DAG manualmente con el botÃ³n â–¶ï¸

---

## ğŸ“Š Ver reporte de calidad de datos (Elementary)

Para generar y visualizar el reporte HTML:

1. AsegÃºrate de tener montado este volumen en `docker-compose.yml`:

```yaml
volumes:
  - ./elementary_report:/home/airflow/.elementary/report
```

2. Ejecuta el DAG hasta el final (incluso si hay tests fallidos)

3. Abre el reporte en tu navegador:

```bash
open ./elementary_report/index.html    # macOS
# o
xdg-open ./elementary_report/index.html  # Linux
```

---

## ğŸ§¯ Apagar el entorno

### Detener servicios sin borrar datos:

```bash
docker compose down
```

### Detener y borrar volÃºmenes (âš ï¸ perderÃ¡s datos de Airflow y Postgres):

```bash
docker compose down -v
```

---

## ğŸ“ Flujo resumen

| Escenario                      | Comando                               |
| ------------------------------ | ------------------------------------- |
| ğŸ— Primer uso o limpieza total | `build` â†’ `up airflow-init` â†’ `up -d` |
| ğŸƒâ€â™‚ï¸ Uso diario               | `up -d`                               |
| â–¶ï¸ Ejecutar DAG                | Desde la UI de Airflow                |
| ğŸ“ˆ Ver reporte HTML            | `open ./elementary_report/index.html` |
| â¹ Detener entorno              | `docker compose down`                 |
| ğŸ”¥ Detener y borrar todo       | `docker compose down -v`              |
---


# ğŸ§­ GuÃ­a: CÃ³mo usar un solo `profiles.yml` para dbt y Elementary en local y en Airflow (Docker)

## ğŸ§© SituaciÃ³n

Cuando trabajas con `dbt` y `elementary` tanto en tu terminal local como en Airflow (que corre dentro de Docker), te enfrentas a este problema comÃºn:

* **Localmente**, la base de datos Postgres se accede por `localhost`.
* **En Docker (Airflow)**, esa misma base se accede por el nombre del servicio Docker, normalmente `postgres`.

Si usas `localhost` en `profiles.yml`, Airflow no puede conectar.
Si usas `postgres`, tÃº no puedes conectar desde tu terminal local.

## âŒ Enfoques incorrectos

* Mantener dos perfiles (`dbt_project_docker` y `dbt_project_local`) â†’ ğŸ” duplicaciÃ³n innecesaria
* Cambiar el `host` manualmente cada vez â†’ ğŸ˜© propenso a errores

## âœ… SoluciÃ³n

Usar `env_var()` en el `host` del `profiles.yml`, lo que te permite:

* Ejecutar localmente sin hacer nada adicional.
* Hacer que Airflow pase la variable `DBT_HOST=postgres` al entorno.

Esto permite que ambos entornos compartan un Ãºnico perfil **dinÃ¡mico** y consistente.

## ğŸ› ï¸ Â¿QuÃ© hay que hacer?

1. En `profiles.yml`, usar:

   ```yaml
   host: "{{ env_var('DBT_HOST', 'localhost') }}"
   ```

2. En tu DAG de Airflow, pasar la variable:

   ```python
   env = {
     "DBT_PROJECT_DIR": "/opt/dbt_project",
     "PATH": "...",
     "DBT_HOST": "postgres"  # <- clave para funcionar en Docker
   }
   ```

3. Ejecutar `dbt` y `edr` normalmente desde terminal:

   ```bash
   dbt build --profile dbt_project
   edr report --profile elementary
   ```

   > âœ” No hace falta exportar nada, porque `localhost` se usa por defecto.

4. Airflow seguirÃ¡ usando el mismo perfil, pero con `host: postgres`.

## ğŸ“Œ Resultado

* ğŸ¯ Un solo `profiles.yml` vÃ¡lido para todos los entornos
* âš™ï¸ Compatible con `dbt` y `elementary`
* ğŸ“¦ Sin duplicaciÃ³n ni ediciÃ³n manual
* ğŸ§˜â€â™‚ï¸ Sin necesidad de recordar en quÃ© entorno estÃ¡s

---


